{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luQTd157jfy8",
        "outputId": "2dacf4e9-52e7-48b2-c328-1e6350377e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the file\n",
        "file_path = '/content/drive/My Drive/papers/structuring_dictionaries/ru_kg_udahin.txt'\n",
        "\n",
        "# Read the file content\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    entries = file.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate lengths of each entry\n",
        "entry_lengths = [len(entry) for entry in entries]\n",
        "\n",
        "# Group entries by their lengths\n",
        "length_groups = defaultdict(list)\n",
        "for entry, length in zip(entries, entry_lengths):\n",
        "    length_groups[length].append(entry)\n"
      ],
      "metadata": {
        "id": "-Sc7MoxzkgCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Determine the total number of entries\n",
        "total_entries = len(entries)\n",
        "\n",
        "# Determine the number of entries to sample\n",
        "sample_size = 520\n",
        "\n",
        "# Calculate proportions\n",
        "proportions = {length: len(group) / total_entries for length, group in length_groups.items()}\n",
        "\n",
        "# Sample proportionally\n",
        "sampled_entries = []\n",
        "for length, group in length_groups.items():\n",
        "    n_samples = int(proportions[length] * sample_size)\n",
        "    sampled_entries.extend(random.sample(group, n_samples))\n",
        "\n",
        "# If there are any discrepancies due to rounding, adjust the sample size accordingly\n",
        "while len(sampled_entries) < sample_size:\n",
        "    additional_sample = random.choice(entries)\n",
        "    sampled_entries.append(additional_sample)\n",
        "\n",
        "# Shuffle the sampled entries to ensure randomness\n",
        "random.shuffle(sampled_entries)\n",
        "\n",
        "# Print or save the sampled entries\n",
        "print(sampled_entries[:10])  # Example to show the first 10 sampled entries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GL0gRYl3mN6b",
        "outputId": "a3a8ce5e-4583-4fa3-f42a-b49d2e17c845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['чукчанка\\tженск. р. к чукча.\\n', 'чесальный,\\t\\xadая, -ое\\\\nтароочу, тарагыч;\\\\nчесальная машина тарагыч машина.\\n', 'босиком\\tнареч.\\\\nжыңайлак, жыңалаяк.\\n', 'легально\\tнареч.\\\\nлегалдуу (закондуу, законго ылайык, ашкере).\\n', 'выбывать\\tнесов.\\\\nсм. выбыть.\\n', 'потворщица\\tженск. р. к потворщик.\\n', 'маргаритка\\tж.\\\\nмаргаритка (гүлү жылдызга окшош өсүмдүктүн бир түрү).\\n', 'игольник\\tм.\\\\nийне сайгыч же ийне салгыч куту.\\n', 'непривычка\\tж. разг.\\\\nкөнбөгөндүк, адаттанбагандык, тажрыйбасыздык;\\\\nс непривычки көнбөгөндүктөн, адаттанбагандыктан.\\n', 'вполголоса\\tнареч.\\\\nкүбүрөп, акырын үн менен, кыңылдап;\\\\nпеть вполголоса кыңылдап ырдоо.\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_entries[-2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "votqglGEmOmq",
        "outputId": "e954af05-3893-456e-eae8-16dc32e0e5a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'отсохнуть\\tсов.\\\\n1. куурап түшүү;\\\\nветка отсохла бутак куурап түштү;\\\\n2. разг. сенек болуп кууроо;\\\\nу него рука отсохла анын колу сенек болуп, куурап калды.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the list into two halves\n",
        "half_size = len(sampled_entries) // 2\n",
        "sampled_entries_part1 = sampled_entries[:half_size]\n",
        "sampled_entries_part2 = sampled_entries[half_size:]\n",
        "\n",
        "# Save the sampled entries to two different files\n",
        "output_path_part1 = '/content/drive/My Drive/papers/structuring_dictionaries/sample_part1.txt'\n",
        "output_path_part2 = '/content/drive/My Drive/papers/structuring_dictionaries/sample_part2.txt'\n",
        "\n",
        "with open(output_path_part1, 'w', encoding='utf-8') as file1:\n",
        "    file1.write('\\n'.join(sampled_entries_part1))\n",
        "\n",
        "with open(output_path_part2, 'w', encoding='utf-8') as file2:\n",
        "    file2.write('\\n'.join(sampled_entries_part2))\n",
        "\n",
        "print(f\"Sampled entries saved to {output_path_part1} and {output_path_part2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS4dP_k9mV9q",
        "outputId": "ee35357f-581e-49f1-8cf5-e6099c5c5f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled entries saved to /content/drive/My Drive/papers/structuring_dictionaries/sample_part1.txt and /content/drive/My Drive/papers/structuring_dictionaries/sample_part2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(entry_lengths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAVVWEJLqEjn",
        "outputId": "f6593ed0-4dee-4076-9bdc-a88f14056329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "935"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min(entry_lengths), max(entry_lengths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PbyWtnvqs2X",
        "outputId": "f3af0108-faca-411f-daa2-5871be96470e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 6603)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stratified buckets\n",
        "\n",
        "We create buckets based on length ranges and then perform stratified sampling within these buckets.\n",
        "This way we make sure that each bucket is proportionally represented in the sample and maintain the overall distribution of entry lengths.\n",
        "\n",
        "Algorithmic steps to do that:\n",
        "\n",
        "    Calculate the length of each entry.\n",
        "    Define 100 buckets based on the range of lengths.\n",
        "    Group entries into these buckets.\n",
        "    Calculate the proportion of entries in each bucket.\n",
        "    Sample proportionally from each bucket.\n",
        "\n",
        "Complete script implementing this:"
      ],
      "metadata": {
        "id": "C0XWDpsuruVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the file\n",
        "file_path = '/content/drive/My Drive/papers/structuring_dictionaries/ru_kg_udahin.txt'\n",
        "\n",
        "# Read the file content\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    entries = file.readlines()\n",
        "\n",
        "# Strip entries to remove any leading/trailing whitespaces\n",
        "entries = [entry.strip() for entry in entries]\n",
        "\n",
        "# Calculate lengths of each entry\n",
        "entry_lengths = [len(entry) for entry in entries]\n",
        "\n",
        "# Determine min and max lengths\n",
        "min_length = min(entry_lengths)\n",
        "max_length = max(entry_lengths)\n",
        "\n",
        "# Define 100 buckets based on the range of lengths\n",
        "num_buckets = 100\n",
        "bucket_size = (max_length - min_length) / num_buckets\n",
        "buckets = defaultdict(list)\n",
        "\n",
        "for entry, length in zip(entries, entry_lengths):\n",
        "    bucket_index = int((length - min_length) // bucket_size)\n",
        "    buckets[bucket_index].append(entry)\n",
        "\n",
        "# Determine the total number of entries\n",
        "total_entries = len(entries)\n",
        "\n",
        "# Determine the number of entries to sample\n",
        "sample_size = 500\n",
        "\n",
        "# Calculate proportions\n",
        "proportions = {bucket: len(group) / total_entries for bucket, group in buckets.items()}\n",
        "\n",
        "# Sample proportionally from each bucket\n",
        "sampled_entries = []\n",
        "for bucket, group in buckets.items():\n",
        "    n_samples = int(proportions[bucket] * sample_size)\n",
        "    sampled_entries.extend(random.sample(group, min(n_samples, len(group))))\n",
        "\n",
        "# Adjust sample size if necessary\n",
        "while len(sampled_entries) < sample_size:\n",
        "    additional_sample = random.choice(entries)\n",
        "    if additional_sample not in sampled_entries:\n",
        "        sampled_entries.append(additional_sample)\n",
        "\n",
        "# Shuffle the sampled entries to ensure randomness\n",
        "random.shuffle(sampled_entries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hiy8haRrvIV",
        "outputId": "658cbbdc-e40b-4726-8459-ee1bef337fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 500 sentences"
      ],
      "metadata": {
        "id": "RbIu4kRVw7dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the list into two halves\n",
        "half_size = len(sampled_entries) // 2\n",
        "sampled_entries_part1 = sampled_entries[:half_size]\n",
        "sampled_entries_part2 = sampled_entries[half_size:]\n",
        "\n",
        "# Save the sampled entries to two different files\n",
        "output_path_part1 = '/content/drive/My Drive/papers/structuring_dictionaries/sample_part1.txt'\n",
        "output_path_part2 = '/content/drive/My Drive/papers/structuring_dictionaries/sample_part2.txt'\n",
        "\n",
        "with open(output_path_part1, 'w', encoding='utf-8') as file1:\n",
        "    file1.write('\\n'.join(sampled_entries_part1))\n",
        "\n",
        "with open(output_path_part2, 'w', encoding='utf-8') as file2:\n",
        "    file2.write('\\n'.join(sampled_entries_part2))\n",
        "\n",
        "print(f\"Sampled entries saved to {output_path_part1} and {output_path_part2}\")"
      ],
      "metadata": {
        "id": "X8sYWwdDr1Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12 000 sentences"
      ],
      "metadata": {
        "id": "TxNilDWpxIu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "\n",
        "def mount_drive():\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "def read_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        entries = file.readlines()\n",
        "    return [entry.strip() for entry in entries]\n",
        "\n",
        "def calculate_entry_lengths(entries):\n",
        "    return [len(entry) for entry in entries]\n",
        "\n",
        "def create_buckets(entries, entry_lengths, num_buckets=100):\n",
        "    min_length = min(entry_lengths)\n",
        "    max_length = max(entry_lengths)\n",
        "    bucket_size = (max_length - min_length) / num_buckets\n",
        "    buckets = defaultdict(list)\n",
        "\n",
        "    for entry, length in zip(entries, entry_lengths):\n",
        "        bucket_index = int((length - min_length) // bucket_size)\n",
        "        buckets[bucket_index].append(entry)\n",
        "\n",
        "    return buckets\n",
        "\n",
        "def calculate_proportions(buckets, total_entries):\n",
        "    return {bucket: len(group) / total_entries for bucket, group in buckets.items()}\n",
        "\n",
        "def sample_proportionally(buckets, proportions, sample_size):\n",
        "    sampled_entries = []\n",
        "    for bucket, group in buckets.items():\n",
        "        n_samples = int(proportions[bucket] * sample_size)\n",
        "        sampled_entries.extend(random.sample(group, min(n_samples, len(group))))\n",
        "    return sampled_entries\n",
        "\n",
        "def adjust_sample_size(sampled_entries, entries, target_size):\n",
        "    while len(sampled_entries) < target_size:\n",
        "        additional_sample = random.choice(entries)\n",
        "        if additional_sample not in sampled_entries:\n",
        "            sampled_entries.append(additional_sample)\n",
        "    return sampled_entries\n",
        "\n",
        "def main(file_path, sample_size):\n",
        "    mount_drive()\n",
        "    entries = read_file(file_path)\n",
        "    entry_lengths = calculate_entry_lengths(entries)\n",
        "    buckets = create_buckets(entries, entry_lengths)\n",
        "    total_entries = len(entries)\n",
        "    proportions = calculate_proportions(buckets, total_entries)\n",
        "    sampled_entries = sample_proportionally(buckets, proportions, sample_size)\n",
        "    sampled_entries = adjust_sample_size(sampled_entries, entries, sample_size)\n",
        "    random.shuffle(sampled_entries)\n",
        "    return sampled_entries"
      ],
      "metadata": {
        "id": "gKCMxpS4xFCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/papers/structuring_dictionaries/ru_kg_udahin.txt'\n",
        "sample_size = 12000\n",
        "result = main(file_path, sample_size)\n",
        "print(f\"Үлгүнүн өлчөмү: {len(result)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN3c1TjPx3vC",
        "outputId": "fee0ad19-3fd6-45e6-d0ae-5f6b4eec8bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Үлгүнүн өлчөмү: 12000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the sampled entries to a file\n",
        "output_path = '/content/drive/My Drive/papers/structuring_dictionaries/12000.txt'\n",
        "\n",
        "with open(output_path, 'w', encoding='utf-8') as file1:\n",
        "    file1.write('\\n'.join(result))"
      ],
      "metadata": {
        "id": "bYduajbByKWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9s2V36gLyfZi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}